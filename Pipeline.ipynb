{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The goal is to develop a class named `DiamondModel` that facilitates the training and utilization of machine learning models for predicting diamond prices. The design focuses on flexibility, user-friendliness, and future extensibility. Here are the key components of the approach:\n",
    "\n",
    "1. **Flexibility for Future Enhancements**:\n",
    "   - The class architecture is designed to accommodate the inclusion of new machine learning models easily. This ensures that as new algorithms and techniques become available, they can be integrated with minimal changes to the existing codebase.\n",
    "\n",
    "2. **User-Friendly API**:\n",
    "   - The class is intended to be used by individuals who may not have a deep understanding of data science. Therefore, the interface is kept simple, requiring only the essential parameters. This makes it easier for users to train models and make predictions without needing to delve into complex configurations.\n",
    "\n",
    "3. **Minimal Input Parameters**:\n",
    "   - **File Name**: Users can provide the name of the file containing the training data. The class supports various file formats (e.g., CSV, Excel, JSON) through a flexible file reader mechanism.\n",
    "   - **Model ID**: If users want to use an existing pre-trained model, they can provide the model's unique identifier. This allows for easy loading and usage of saved models.\n",
    "   - **Model Type**: Users need to specify the type of model they wish to train. Currently, options include `LinearRegression` and `XGBRegressor`. The class validates the input and initializes the appropriate model accordingly.\n",
    "\n",
    "4. **Data Handling and Cleaning**:\n",
    "   - The class includes methods for loading data from different file formats, cleaning the data, and preprocessing it for model training. This ensures that the data is in the right format and free of inconsistencies before being used to train the model.\n",
    "\n",
    "5. **Model Training and Hyperparameter Optimization**:\n",
    "   - The class provides functionality for training the specified model using the provided data. For more complex models like `XGBRegressor`, it integrates with `optuna` to perform hyperparameter optimization, ensuring the best possible model performance.\n",
    "\n",
    "6. **Visualization and Evaluation**:\n",
    "   - Several visualization methods are included to help users understand the data and the model's performance. This includes plotting scatter matrices, histograms, and other relevant visualizations.\n",
    "   - The class also offers methods to evaluate the model's performance using metrics like mean absolute error (MAE).\n",
    "\n",
    "7. **Prediction and Similarity Analysis**:\n",
    "   - Once trained, the model can be used to predict diamond prices for new data. The class includes methods to handle these predictions seamlessly.\n",
    "   - Additionally, it provides functionality to find similar diamond samples based on specified characteristics, aiding in comparative analysis.\n",
    "\n",
    "This approach ensures that the `DiamondModel` class is robust, versatile, and easy to use, catering to both novice users and experienced data scientists looking for a streamlined solution for diamond price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a class `DiamondModel` for handling diamond price prediction using machine learning. It leverages various libraries such as `pandas` for data manipulation, `scikit-learn` for model evaluation, `xgboost` for advanced regression, `joblib` for model persistence, `optuna` for hyperparameter optimization, and visualization libraries like `matplotlib` and `plotly`.\n",
    "\n",
    "#### Libraries\n",
    "- `pandas`: Data manipulation and analysis.\n",
    "- `scikit-learn`: Machine learning utilities and evaluation metrics.\n",
    "- `xgboost`: Extreme Gradient Boosting library for regression.\n",
    "- `joblib`: Model saving and loading.\n",
    "- `optuna`: Hyperparameter optimization.\n",
    "- `matplotlib` and `plotly`: Data visualization.\n",
    "\n",
    "#### File Readers\n",
    "A dictionary maps file extensions to their corresponding `pandas` file reading functions, allowing for flexible data loading from various formats (e.g., CSV, Excel, JSON, etc.).\n",
    "\n",
    "#### DiamondModel Class\n",
    "The core of the script is the `DiamondModel` class, which provides functionality for training and using machine learning models for diamond price prediction.\n",
    "\n",
    "##### Initialization\n",
    "The class is initialized with either training data or an ID to load an existing model. It supports `LinearRegression` and `XGBRegressor` models. The initialization process:\n",
    "1. Validates input parameters.\n",
    "2. Loads data from a file if a file path is provided.\n",
    "3. Loads an existing model if an ID is provided.\n",
    "4. Initializes the chosen model.\n",
    "\n",
    "##### Data Cleaning\n",
    "The `clean_data` method processes the data by:\n",
    "1. Dropping specified columns.\n",
    "2. Removing rows with missing values or invalid entries (e.g., zero dimensions or negative prices).\n",
    "3. Generating dummy variables for categorical columns.\n",
    "\n",
    "##### Visualization Methods\n",
    "Several methods provide data visualization capabilities:\n",
    "- `visualize_scatter_matrix`: Plots a scatter matrix to show correlations between numeric columns.\n",
    "- `visualize_histogram`: Displays histograms of the dataset’s numeric columns.\n",
    "- `visualize_diamond_prices_by`: Creates violin plots to visualize diamond prices based on a specified column (e.g., cut, color, clarity).\n",
    "\n",
    "##### Model Training and Prediction\n",
    "The `train_model` method trains the model using the cleaned data and returns the mean absolute error (MAE) as an evaluation metric. It involves:\n",
    "1. Data preparation: Splitting data into training and testing sets.\n",
    "2. Model training: If using `XGBRegressor`, it finds the best hyperparameters using `optuna`.\n",
    "3. Model evaluation: Predicting prices on the test set and calculating MAE.\n",
    "4. Model persistence: Saving the trained model for future use.\n",
    "\n",
    "The `predict` method uses the trained model to predict diamond prices for new data.\n",
    "\n",
    "##### Hyperparameter Optimization\n",
    "The class uses `optuna` for hyperparameter optimization. The `objective` method defines the model's hyperparameters to tune, and the `find_best_hyperparameters` method runs the optimization process.\n",
    "\n",
    "##### Plotting Predictions\n",
    "The `plot_predictions_vs_actual` method plots the predicted prices against the actual prices to visually assess the model's performance.\n",
    "\n",
    "##### Similar Samples\n",
    "The `get_similar_samples` method retrieves similar diamond samples based on specified characteristics (carat, cut, color, clarity).\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "To use the `DiamondModel` class, you can initialize it with data and train a model as follows:\n",
    "\n",
    "```python\n",
    "# Initialize the model with data\n",
    "model = DiamondModel(datas='diamonds.csv', model='XGBRegressor')\n",
    "\n",
    "# Clean the data\n",
    "model.clean_data()\n",
    "\n",
    "# Train the model and get the mean absolute error\n",
    "mae = model.train_model()\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "\n",
    "# Visualize the scatter matrix\n",
    "model.visualize_scatter_matrix()\n",
    "\n",
    "# Predict prices for new data\n",
    "new_data = pd.DataFrame({\n",
    "    # Define new data here\n",
    "})\n",
    "predictions = model.predict(new_data)\n",
    "print(predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "import uuid\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import optuna\n",
    "from typing import Union\n",
    "\n",
    "file_readers = {\n",
    "    '.csv': pd.read_csv,\n",
    "    '.xlsx': pd.read_excel,\n",
    "    '.xls': pd.read_excel,\n",
    "    '.json': pd.read_json,\n",
    "    '.html': pd.read_html,\n",
    "    '.hdf': pd.read_hdf,\n",
    "    '.feather': pd.read_feather,\n",
    "    '.parquet': pd.read_parquet,\n",
    "    '.pkl': pd.read_pickle,\n",
    "    '.sql': pd.read_sql,\n",
    "    '.stata': pd.read_stata,\n",
    "    '.sas': pd.read_sas,\n",
    "    '.spss': pd.read_spss,\n",
    "    '.dta': pd.read_stata,\n",
    "    '.orc': pd.read_orc,\n",
    "    '.gbq': pd.read_gbq\n",
    "}\n",
    "\n",
    "class DiamondModel:\n",
    "    \"\"\"\n",
    "    Initialize the model either with data to train the model or an id to load an existing model.\n",
    "    \n",
    "    Params:\n",
    "    datas: Union[pd.DataFrame, str]: The data to train the model, either a pandas DataFrame or a path to a file.\n",
    "    id: str: The id of the model to load.\n",
    "    model: str: The model to train the data. Options:\n",
    "        - LinearRegression\n",
    "        - XGBRegressor\n",
    "    \"\"\"\n",
    "    def __init__(self, datas: Union[pd.DataFrame, str]=None, id: str=None, model: str=None) -> None:\n",
    "        self.datas = None\n",
    "        self.model = None\n",
    "        self.modelling_algorithms = {\n",
    "            \"LinearRegression\": LinearRegression,\n",
    "            \"XGBRegressor\": XGBRegressor\n",
    "        }\n",
    "        self.id = uuid.uuid4().hex\n",
    "        self.model_name = model\n",
    "\n",
    "        if datas is None and id is None:\n",
    "            raise ValueError(\"You must provide data to train the model or an id to load the model\")\n",
    "        if id is None and model is None:\n",
    "            raise ValueError(\"You must decide a model to train the data\")\n",
    "        if model and model not in self.modelling_algorithms:\n",
    "            raise ValueError(\"The model provided is not yet supported\")\n",
    "        \n",
    "        if isinstance(datas, str):\n",
    "            self.datas = self._load_data_from_file(datas)\n",
    "        elif isinstance(datas, pd.DataFrame):\n",
    "            self.datas = datas\n",
    "\n",
    "        if id:\n",
    "            self._load(id)\n",
    "        \n",
    "        if self.datas is None:\n",
    "            raise ValueError(\"The data could not be loaded\")\n",
    "\n",
    "        if model:\n",
    "            self.model = self.modelling_algorithms[model]()\n",
    "\n",
    "    @staticmethod\n",
    "    def objective(x_train_xbg, y_train_xbg, trial: optuna.trial.Trial) -> float:\n",
    "        # Define hyperparameters to tune\n",
    "        param = {\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.7]),\n",
    "            'subsample': trial.suggest_categorical('subsample', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-8, 1.0, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "            'random_state': 42,\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "\n",
    "        # Split the training data into training and validation sets\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train_xbg, y_train_xbg, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train the model\n",
    "        model = xgboost.XGBRegressor(**param)\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = model.predict(x_val)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "\n",
    "        return mae\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_hyperparameters(x_train_xbg, y_train_xbg, n_trials: int=100) -> dict:\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: DiamondModel.objective(x_train_xbg, y_train_xbg, trial), n_trials=n_trials)\n",
    "        return study.best_params\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_gof(y_true: pd.Series, y_pred: pd.Series):\n",
    "        plt.plot(y_true, y_pred, '.')\n",
    "        plt.plot(y_true, y_true, linewidth=3, c='black')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.show()\n",
    "\n",
    "    def _load_data_from_file(self, file_path: str) -> pd.DataFrame:\n",
    "        for extension, reader in file_readers.items():\n",
    "            if file_path.endswith(extension):\n",
    "                return reader(file_path)\n",
    "        raise ValueError(f\"Unsupported file extension: {file_path}\")\n",
    "\n",
    "    def _load(self, id: str):\n",
    "        model_path = f'models/{id}.pkl'\n",
    "        if os.path.exists(model_path):\n",
    "            self.model = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"The model does not exist\")\n",
    "\n",
    "    def _save(self):\n",
    "        joblib.dump(self.model, f'models/{self.id}.pkl')\n",
    "\n",
    "    def clean_data(self, columns_to_drop: list[str] = ['depth', 'table', 'y', 'z']):\n",
    "        \"\"\"\n",
    "        Clean the data by dropping unnecessary columns and rows with missing values.\n",
    "        Params:\n",
    "        columns_to_drop: list[str]: The columns to drop from the data.\n",
    "        \"\"\"\n",
    "        self.datas = self.datas.dropna()\n",
    "        self.datas = self.datas[(self.datas.x * self.datas.y * self.datas.z != 0) & (self.datas.price > 0)]\n",
    "        self.datas = self.datas.reset_index(drop=True)\n",
    "        self.datas_processed = self.datas.drop(columns=columns_to_drop)\n",
    "        self.datas_dummies = pd.get_dummies(self.datas_processed)\n",
    "\n",
    "    def visualize_scatter_matrix(self, save: bool=False) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the correlation between numeric columns in the data.\n",
    "        \"\"\"\n",
    "        pd.plotting.scatter_matrix(self.datas.select_dtypes(include=['number']), figsize=(14, 10))\n",
    "        if save:\n",
    "            plt.savefig(f'visualizations/{self.id}_scatter_matrix.png')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_histogram(self, save: bool=False) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the histogram of the data.\n",
    "        save : bool: Whether to save the plot or not.\n",
    "        \"\"\"\n",
    "        self.datas.hist(bins=100, figsize=(14, 10))\n",
    "        if save:\n",
    "            plt.savefig(f'visualizations/{self.id}_histogram.png')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_diamond_prices_by(self, cut_column: str,save: bool=False) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the prices of diamonds by a specific column.\n",
    "        Params:\n",
    "        cut_column: str: The column to visualize the prices by.\n",
    "        \"\"\"\n",
    "        fig = px.violin(self.datas, x=cut_column, y='price', color=cut_column, title=f'Price by {cut_column}')\n",
    "        fig.show()\n",
    "        if save:\n",
    "            fig.write_html(f'visualizations/{self.id}_price_by_{cut_column}.html')\n",
    "\n",
    "    def train_model(self) -> float:\n",
    "        \"\"\"\n",
    "        Train the model and return the mean absolute error (MAE).\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'datas_dummies'):\n",
    "            warnings.warn(\"The data is not yet cleaned, cleaning it now using the default columns to drop\")\n",
    "            self.clean_data()\n",
    "\n",
    "        X = self.datas_dummies.drop(columns=['price'])\n",
    "        y = self.datas_dummies['price']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        if self.model_name == 'XGBRegressor':\n",
    "            best_params = DiamondModel.find_best_hyperparameters(X_train, y_train)\n",
    "            self.model = self.model(**best_params, enable_categorical=True, random_state=42)\n",
    "        else:\n",
    "            self.model = self.model()\n",
    "\n",
    "        self.model.fit(X_train, y_train)\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        self.predictions = y_pred\n",
    "        self.GT_Y = y_test\n",
    "        self._save()\n",
    "        return mae\n",
    "\n",
    "    def plot_predictions_vs_actual(self, save: bool=False) -> None:\n",
    "        \"\"\"\n",
    "        Plot the predictions vs the actual values.\n",
    "        save: bool: Whether to save the plot or not.\n",
    "        \"\"\"\n",
    "        DiamondModel.plot_gof(self.GT_Y, self.predictions)\n",
    "        if save:\n",
    "            plt.savefig(f'visualizations/{self.id}_predictions_vs_actual.png')\n",
    "        \n",
    "    def predict(self, data: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Predict the prices of diamonds using the trained model.\n",
    "        Params:\n",
    "        data: pd.DataFrame: The data to predict the prices of diamonds.\n",
    "        \"\"\"\n",
    "        return self.model.predict(data)\n",
    "    \n",
    "    def get_similar_samples(self, carat: float, cut: str, color: str, clarity: str, n: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get similar samples to a given diamond.\n",
    "        Params:\n",
    "        carat: float: The carat of the diamond.\n",
    "        cut: str: The cut of the diamond.\n",
    "        color: str: The color of the diamond.\n",
    "        clarity: str: The clarity of the diamond.\n",
    "        n: int: The number of similar samples to return.\n",
    "        \"\"\"\n",
    "        if self.datas is None:\n",
    "            raise ValueError(\"Data is not loaded.\")\n",
    "        if carat <= 0 or n <= 0:\n",
    "            raise ValueError(\"Carat and n must be greater than 0\")\n",
    "        if carat>max(self.datas['carat']) or carat<min(self.datas['carat']):\n",
    "            raise ValueError(\"Carat must be within the range of the dataset\")\n",
    "        if n>len(self.datas):\n",
    "            return self.datas\n",
    "        if clarity not in self.datas['clarity'].unique():\n",
    "            raise ValueError(\"Clarity must be one of the unique values in the dataset\")\n",
    "        if color not in self.datas['color'].unique():\n",
    "            raise ValueError(\"Color must be one of the unique values in the dataset\")\n",
    "        if cut not in self.datas['cut'].unique():\n",
    "            raise ValueError(\"Cut must be one of the unique values in the dataset\")\n",
    "        \n",
    "        \n",
    "        # Filtra il dataset\n",
    "        filtered_data = self.datas[\n",
    "            (self.datas['cut'] == cut) &\n",
    "            (self.datas['color'] == color) &\n",
    "            (self.datas['clarity'] == clarity)\n",
    "        ]\n",
    "        \n",
    "        if filtered_data.empty:\n",
    "            return pd.DataFrame()  # Restituisce un DataFrame vuoto se non ci sono campioni simili\n",
    "\n",
    "        # Calcola la differenza di peso e ordina per similarità di peso\n",
    "        filtered_data['weight_diff'] = (filtered_data['carat'] - carat).abs()\n",
    "        similar_samples = filtered_data.sort_values(by='weight_diff').head(n)\n",
    "        \n",
    "        # Rimuovi la colonna temporanea 'weight_diff'\n",
    "        similar_samples = similar_samples.drop(columns=['weight_diff'])\n",
    "        \n",
    "        return similar_samples    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
